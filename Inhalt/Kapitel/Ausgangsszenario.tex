\chapter{Ausgangsszenario}
\label{chapter:ausgangsszenario}
Nach Darlegung der Methodik im vorangegangenen Kapitel, wird in diesem präsentiert worauf sie angewendet wird.
Es werden die Anforderungen definiert und messbar gemacht, sowie Funktions- und Leistungstests skizziert.
Zur Einordnung der Gewichtung der zu untersuchenden Qualitäten und Heranführung an die Absichten der Agri~Con, wird der Ist-Stand und die gewünschte Integration eines neuen Frameworks ausgeführt.
Zur Einordnung der Arbeit und Einsicht in aktuelle Zusammenhänge, endet dieses Kapitel mit der Darlegung von wissenschaftlichen Dokumenten, welche im Zusammenhang dieser Arbeit stehen.

\section{Anforderungen}
\label{Anforderungen}
% kartografisches Produkt
Aktuelle Möglichkeiten der Datenerfassung über Sensoren und moderne Probenahmegeräte führen zu mehr und mehr Datensätzen, die für einen Landwirtschaftsbetrieb ausgewertet werden müssen. Darüber hinaus besteht die Notwendigkeit, Daten Jahres übergreifend und betriebsübergreifend auszuwerten, um pflanzenbauliche Zusammenhänge über statistische Methoden untersuchen zu können.
In den letzten 3 Jahren wurde beispielsweise nur zum Thema N-Versorgung\footnote{N-Versorgung betrifft die Stickstoffdüngung und -aufnahme.} für einen Betrieb etwa 800 Datensätze mit 1,9 Mio. Einträgen erfasst. Alle diese Daten haben einen räumlichen Bezug, sie müssen weiterverarbeitet, kartographisch aufbereitet und dargestellt werden.

Daraus ergeben sich verschiedenen Anforderungen an die Technologie, die für die Verarbeitung, Analyse und Darstellung verwendet wird:
\begin{itemize}
\item PostgreSQL mit PostGIS zum Datenimport und -export nutzbar
\item Gruppierung und Filterung mit geringer Laufzeit
\item Parallele Berechnung über große Datenmengen mit geringer Laufzeit
\item Räumliche Berechnungen wie Verschneidung und Overlays
%\item  Unterschiedliche Prinzipien der Kartengenerierung, hier dynamisches rendern aus dem Datenbestand zur Laufzeit oder dynamisches rendern bei Dateneingang wodurch vorgerenderte Karten bereitstehen % caches wirklich mit untersuchen? wenn ja in Schnittstellen aufnehmen
\item Nutzbare Schnittstelle zur Darstellung mit dem \Gls{umn}
\end{itemize}

% TODO: ergänzen, dass Anforderungen von Ist-Stand und möglichen Verbesserungen herrühren

% Eventuell technische Sicht extra darstellen, um Eignung der Systeme besser herausarbeiten zu können
Konkret handelt es sich bei den Eingangsdaten um folgende:
\begin{itemize}
\item Pflanzenbauliche Daten\footnote{Pflanzenbauliche Daten sind: Sensoren, Bodenuntersuchung, \Gls{bonitur} und Logger}: Punktdaten
\item Basisdaten wie Feldgrenzen sowie Interpolationen\footnote{Interpolationen als aufbereitetes Ergebnis von Daten der Nährstoffverteilung und Bodenscanner.}: Vektordaten
\item Externe Satelliteninformationen und Multispektralanalysen: Rasterdaten
\end{itemize}

\subsection{Softwarequalität}
\label{softwarequalität}
%TODO: warum hier die qualitätsmerkmale
Qualitätsmerkmale sind nach DIN 9126\footnote{DIN 9126 wurde durch ISO/IEC 25000 ersetzt, jedoch sind beide nur proprietär verfügbar.} in \cite[S.258 f.]{book:lehrbuchsoftware} Funktionalität, Zuverlässigkeit, Benutzbarkeit, Effizienz, Änderbarkeit und Übertragbarkeit.
Diese Merkmale werden durch Qualitätskriterien für jeden Anwendungsfall konkretisiert.
Nachfolgend werden die Qualitätsmerkmale für diesen Anwendungsfall dargestellt und darauf die zu untersuchenden aufgelistet.
Da die zu analysierenden Systeme eine Datenbank beinhalten, welche mit räumlichen Datentypen arbeitet, wurde die im Anhang C von \cite{book:objdbs} enthaltene Checkliste zur Auswahl eines \Gls{odbms} berücksichtigt.

\textbf{Funktionalität}\\
Das System stellt alle geforderten Funktionen mit den definierten Eigenschaften zur Verfügung.
\begin{itemize}
\item \textbf{Richtigkeit:} Ergebnisse sind korrekt oder ausreichend genau.
Die originalen räumlichen Daten werden von Sensoren erfasst, in wessen Toleranzbereich die Ergebnisse der Verarbeitung durch das Framework liegen müssen.
\item \textbf{Interoperabilität:} Es sind Schnittstellen zur Ein- und Ausgabe vorhanden. Dabei soll es sich um PostgreSQL Import sowie PostgreSQL und \Gls{umn} Export handeln.
\item \textbf{Funktionsumfang:} Mindestens die benannte und essentielle Menge an Funktionalitäten wird bereitgestellt. Dazu zählt:
Parallele Verarbeitung, Gruppierungs-,\\Filter-, Verschneidungs- sowie Overlayfunktionen, Geostatistik und Umrechnung zwischen Koordinatensystemen und -formaten.
Außerdem sind vorhandene Datentypen und Schemaversionierung von Interesse.
\item \textbf{Ordnungsmäßigkeit:} Die Implementierung des Systems und dessen Funktionen erfüllt Normen, Vereinbarungen, gesetzliche Bestimmungen und andere Vorschriften. Hierzu ist zu nennen, dass besonders Berechnungsfunktionen nach mathematischen Gesetzen implementiert sein müssen. Konkret sind Berechnungen der räumlichen Verarbeitung nach anerkannten definierten Algorithmen durchzuführen. Weiterhin sind Definitionen der Koordinatenreferenzsysteme\footnote{Koordinatenreferenzsysteme siehe \Glspl{epsg-code}}, die mathematisch korrekte deterministische sowie stochastische Interpolation und Algorithmen nach Krige einzuhalten. Auch allgemeine Anforderungen an \Gls{dbms} wie Integritätssicherung, Datensicherheit, Mehrbenutzerbetrieb, Datenunabhängigkeit und Zugangssicherung müssen erfüllt werden.
\end{itemize}
\ \\
%
\textbf{Zuverlässigkeit}\\
Nach \cite[S.259]{book:lehrbuchsoftware} wird Zuverlässigkeit als die Fähigkeit einer Software definiert,  ihr Leistungsniveau unter festgelegten Bedingungen über einen festgelegten Zeitraum zu bewahren.
%Nutzung von Tools zur Überwachung und Konfiguration immanent.
\begin{itemize}
\item \textbf{Fehlertoleranz:} Das System sollte auftretende Fehler des Tagesgeschäftes abfangen und weiterarbeiten. Besonders Fehler in den Quelldaten können zu Fehlern während der Ausführung von Berechnungen führen, was per s\'{e} abgefangen werden muss.
\item \textbf{Wiederherstellbarkeit:} Auch die Möglichkeit bei einem schwerwiegendem Fehler den Zustand der abgebrochenen Operationen wiederherzustellen ist ein zu betrachtendes Qualitätskriterium.
\item \textbf{\Gls{mttf}:} Diese statische Kenngröße der erfahrungsgemäßen mittleren Lebensdauer ist für kritische Systeme relevant.
\end{itemize}
\ \\
%
\textbf{Benutzbarkeit}\\
Qualität des Zugangs für Benutzer sowie Eignung für eine oder mehrere Benutzergruppen.
\begin{itemize}
\item \textbf{Verständlichkeit} 
\item \textbf{Bedienbarkeit}
\item \textbf{Dokumentation:} Eine ausführliche, aktuelle und korrekte Dokumentation ist Voraussetzung zur produktiven Verwendung.
\item \textbf{Eignung:} Die angestrebte Benutzergruppe muss mit der aktuellen Benutzergruppe übereinstimmen. Die aktuelle Benutzergruppe ist Programmierer bzw. Administrator.
\end{itemize}
\ \\
%
\textbf{Effizienz}\\
Effizienz ist das Verhältnis zwischen Auslastung der Hardware und erfolgreich bearbeiteten Aufgaben. Nach \cite[S.21]{book:Leistungsanalyse} ist Leistung paralleler Programme das Verhältnis des Speedups zur Anzahl der verwendeten Prozessoren. Wobei Speedup als Verhältnis der Ausführungszeiten zwischen der auf N Prozessoren ausgeführten parallelen Version eines Programms und der sequentiellen Version des Programmes definiert ist. Diese Definitionen treffen für die zu untersuchenden Systeme zu, da es sich um parallelisierende \Gls{gis} handelt.
\begin{itemize}
\item \textbf{Zeitverhalten:} Oder auch Laufzeitverhalten genannt, dient allgemein zur Darstellung des Durchsatzes. Die Skalierung des Systems zählt hier dazu. Dies wird speziell durch zusätzliche Leistungstests beurteilt.
\item \textbf{Verbrauchsverhalten:} Das Verhältnis aus erbrachter Leistung und dem dafür notwendig gewesenen Aufwand in Form von Hardwarenutzung.
\item \textbf{Skalierbarkeit:} Anzahl der zu verwendenden Computer um nach dem Speedup eine Effizienzsteigerung im Gegensatz zum Einsatz bei einem Computer zu erreichen.
\end{itemize}
\ \\
%
\textbf{Änderbarkeit}\\
Aufwand zur Verbesserung oder Anpassung der Umgebung und der Spezifikationen, auch Wartungsaufwand genannt.
\begin{itemize}
\item \textbf{Analysierbarkeit:} \glqq Aufwand, um Mängel oder Ursachen von Versagen zu diagnostizieren oder um änderungsbedüftige Teile zu bestimmen.\grqq\ \cite[S. 260]{book:lehrbuchsoftware}
\item \textbf{Modifizierbarkeit:} Notwendiger Aufwand für Änderungen zum Ziele der Verbesserung und Fehlerbehebung.
\item \textbf{Stabilität:} Wahrscheinlichkeit von ungewollten Auswirkungen durch Änderungen.
\item \textbf{Prüfbarkeit:} Oder Testbarkeit als Merkmal, welches die Möglichkeiten und den Aufwand zum testen der originalen und geänderten Systeme darstellt.
\end{itemize}
\newpage
\textbf{Übertragbarkeit}\\
Die Fähigkeit das System auf andere Hard- und Software sowie andere Vorgehensweisen zu migrieren.
\begin{itemize}
\item \textbf{Anpassbarkeit:} Möglichkeiten des unveränderten Systems Änderungen vorzunehmen.
\item \textbf{Installierbarkeit:} Systemvoraussetzung und Aufwand zur Installation des Systems.
\end{itemize}
\ \\
%
\textbf{Nichttechnische Kriterien}\\
Erweiterte Qualitätskriterien, welche nicht nach der DIN 9126 zugeordnet werden können.
\begin{itemize}
\item \textbf{Herstellerfirma und Produkt:} Dazu zählt die Marktposition, der Preis, die Produktplanung und der Service.
\end{itemize}
\ \\
%
Die zu untersuchenden Qualitätskriterien für die Softwareauswahl sind Funktionsumfang, Fehlertoleranz, Dokumentation, Zeitverhalten, Analysier- und Modifizierbarkeit.


\subsection{Qualitätsmetriken}
\label{qualitätsmetriken}
%TODO: essentielles markieren oder stärker wichten
Zu den wichtigen Qualitätsmerkmalen aus Kapitel \ref{softwarequalität} sind nachfolgend Kriterien definiert, sowie je eine Bewertungsvorschrift und die geforderte Mindestbewertung für den Anwendungsfall angegeben.

\textbf{Richtigkeit:}\\
Berechnungen sind zu 100\% korrekt. Ausnahme ist dabei die Berechnung von Koordinaten. Dabei haben die Ergebnisse bis acht Stellen nach dem Komma korrekt zu sein.
Die statische Abbildung ist dabei $[korrekt, nicht\ korrekt]\ nach\ [1, 0]$ und die geforderte Mindestbewertung 1.

\textbf{Interoperabilität:}\\
Ist der Import und Export von räumlichen Daten aus PostgreSQL sowie eine Anbindungsmöglichkeit an den \Gls{umn}.
Statische Abbildung:\\
$[Datenschnittstelle\ und\ UMN\ Schnittstelle\ vorhanden,Datenschnittstelle\ vorhanden,$\\$UMN\ Schnittstelle\ vorhanden,keine\ Schnittstelle\ vorhanden]\ nach\ [12,7,5,0]$\\
Der Bereich bis 12 soll die Wichtigkeit des Vorhandenseins der Schnittstellen verdeutlichen.
Die geforderte Mindestbewertung ist 7.

\textbf{Funktionsumfang:}\\
Tabelle \ref{table:funktionsumfang} gibt die Wertung bei Existenz der einzelnen Funktionen wieder.
Existiert die Funktion nicht, ist die Wertung null.
Ein Zwischenwert bei eingeschränkter Funktionalität ist möglich.
Die maximale Wertung ist dabei 61. %\FPtrunc\Gesamtsumme\Gesamtsumme{0}\FPprint\Gesamtsumme\\
Es müssen mindestens räumliche Datentypen, Filterfunktionen und parallele Verarbeitung vorhanden sein.
\begin{table}[h]
\centering
\begin{tabular}{|l|c|}
\hline
\textbf{Funktion} & \textbf{Wertung} \\ \hline
Parallele Verarbeitung & \psum{2} \\ \hline
Räumliche Datentypen & \psum{14} \\ \hline
Umrechnung zwischen Koordinatensystemen & \psum{10} \\ \hline
Gruppierungsfunktionen & \psum{10} \\ \hline
Verschneidungsfunktionen & \psum{4} \\ \hline
Overlayfunktionen & \psum{4} \\ \hline
Geostatistik & \psum{6} \\ \hline
Filterfunktionen & \psum{10} \\ \hline
Schemaversionierung & \psum{1} \\ \hline
\end{tabular}
\caption{Wertungstabelle Funktionsumfang}
\label{table:funktionsumfang}
\end{table}

\textbf{Fehlertoleranz:}\\
Es gilt zu messen, ob Fehler bei einer Berechnung andere verschränkt gleichzeitig laufende Berechnungen beeinträchtigen.
Aus diesem Grund wird Unabhängigkeit auf eins und Abhängigkeit auf null abgebildet.
Die geforderte Mindestbewertung ist eins.

\textbf{Dokumentation:}\\
Vorhandene Dokumentation ist nach einzelnen Themen zu bewerten.
Dabei kann ein maximaler Wert von 13 erreicht werden.
\begin{table}[h]
\centering
\begin{tabular}{|l|l|}
\hline
\textbf{Dokumentation zu} & \textbf{Wertung je Eintrag} \\ \hline
Installation, Zeitverhalten & 1 \\ \hline
Funktionsumfang & 2 \\ \hline
Interoperabilität, Best practise, Anpassbarkeit & 3 \\ \hline
\end{tabular}
\caption{Wertungstabelle Dokumentation}
\label{table:dokumentation}
\end{table}
Die geforderte Mindestbewertung ist sechs.

\textbf{Zeitverhalten:}\\
\label{bf:zeitverhalten}%
%TODO: Werte aus Ist-Stand in Bezug auf Lasttests verwenden
Konkret wird eine Beschleunigung aufwendiger Vorgänge angestrebt.
Die statische Abbildung auf Bezug auf die Laufzeiten des Ist-Standes:\\
$[Zeit\ wird\ \"uberschritten,\ Zeit\ ist\ gleich,\ Zeit\ wird\ unterschritten]\ nach\ [0,1,3]$\\
%Die Laufzeiten sind in Anhang \ref{appendix-A} zu finden.

\textbf{Modifizierbarkeit:}\\
Mögliche Anpassungen des Frameworks hinsichtlich der folgenden Punkte erhöhen den Wert um eins:\\
Verwendung eigener Datentypen, Erstellung eigener Schnittstellen, Erstellung eigener Funktionen, Verwendung der Programmiersprachen Scala oder R, Anlegen eigener Berechnungsvorgängen zur späteren Abarbeitung\\
Die geforderte Mindestbewertung ist abhängig vom Funktionsumfang und der Interoperabilität.
Jedoch sollten fehlende Funktionen und Schnittstellen erstellt werden können.

\subsection{Testfälle}
\label{subsection:testfaelle}
Zur definierten und wiederholbaren Erfassung der Erfüllung von speziellen Kriterien wie Funktionalität und Zeitverhalten,
folgt die Definition der Funktions- und Lasttests.

\subsubsection{Funktionstests}
Nach der Definition in Kapitel \ref{grundlagen-funktionstests} werden hiermit spezielle Funktionalitäten auf Vorhandensein und die Menge der Schnittstellen sowie der Austauschformate getestet.
Diese Funktionstest sind nicht automatisiert für unterschiedliche Frameworks durchführbar.
Deshalb werden sie manuell anhand der Spezifikation und des Quellcodes durchgeführt und die Ergebnisse tabellarisch festgehalten.

Folgende Schnittstellen sind zu testen:
\begin{itemize}
\item PostgreSQL
\item PostGIS
\item \Gls{umn}
\end{itemize}

Als Austauschformat ist mindestens ein der folgenden Datentypen zum Datenaustausch notwendig:
\begin{itemize}
\item Simple Feature Access\footnote{Simple Feature Access siehe Kapitel \ref{forschungsstand}}
\item Objekte der \Gls{jts}
\item PostGIS geometry
\end{itemize}

Speziell in GIS gilt es die Koordinatenreferenzsysteme zu analysieren.
Die EPSG Codes 3578 und 4326 werden im Anwendungsfall verwendet.

Für folgende Aufgaben sind die Funktionen zu analysieren:
\begin{itemize}
\setlength{\itemsep}{-8pt}
\item Umwandlung zwischen Koordinatensystemen
\item Verschneidung von räumlichen Daten
%\item Geostatistik
\item Interpolation
\item Kriging
\item Topologische Filterung
\item Räumliche Filterung
\end{itemize}
Dabei ist stets zu berücksichtigen, mit welchen Datentypen die Funktionen verwendet werden können.

\subsubsection{Lasttests}
Die Basis für die Erstellung von Lasttests ist Kapitel \ref{iststand-vorgaenge}.
Eine exakte Definition der Lasttests ist in diesem Kapitel nicht möglich, da das zu verwendende Framework noch nicht ausgewählt wurde und somit die darin zu verwendenden Werkzeuge und Daten nicht bekannt sind.
Es werden jedoch die zu testenden Szenarien benannt.
Da es sich bei den zu untersuchenden Frameworks um verteilte Systeme handelt, ist in allen Tests die Verteilung der Daten und die Verteilung der Verarbeitungsschritte zu berücksichtigen.

Ein zu testendes Szenario ist die Aggregation von Daten.
Dabei ist eine Menge von mehreren hundert Megabyte (MB) an Punktdaten mit den gegebenen Werkzeugen abzurufen und die Laufzeit zu messen.
Die Werkzeuge sollten dabei eine Abfragesprache wie SQL und bei Vorhandensein der Schnittstelle der \Gls{umn} sein.
Über eine Abfragesprache sind neben der Laufzeit weitere Informationen abrufbar.
Mit SQL kann das Vorgehen des Query Planers ausgegeben werden, was für die Auswertung herangezogen werden kann.
Die Verwendung des \Gls{umn} zielt auf eine anwendungsnahe Darstellung der Leistungsfähigkeit ab.
Die hohe Leistungsfähigkeit des \Gls{umn} für ein solches Szenario wurde bereits in \cite{ba:kurt} dargestellt, sodass von einer hohen Auslastung des \Gls{dbms} ausgegangen werden kann.

Analog zu diesem ersten Szenario ist ein weiterer Test zur Interpolation bzw. Kriging durchzuführen.
Darin sollen Punktdaten zu Vektordaten mit Interpolation im Rahmen eines Krigings verarbeitet werden.
Damit soll die Verarbeitungsleistung bewertet werden.

%Lasttests zur Persistierung von Eingangsdaten?

\section{Aktuelles System}
\label{IstStand}
%groben Ablauf textuell und grafisch darstellen
% geplanten Einsatz des Prototypen ebenso darstellen 
Die Durchführung einer Softwareauswahl zum teilweisen Ersatz eines bestehenden Systems setzt die Analyse des Systems voraus.
In diesem Unterkapitel wird das aktuelle System sowie der angestrebte Zustand nach Einbau des Prototypen dargestellt.
Firmeninterne Schnittstellen mit dem ist-Stand werden nicht konkretisiert, da sie den Anwendungsfall nicht schneiden.

In Abbildung \ref{fig:iststand} ist die Übersicht des Aufbaus ersichtlich.
Das Herzstück bildet die objektrelationale Datenbank PostgreSQL mit der geografischen Erweiterung PostGIS.
Diese dient zur Datenhaltung und wesentlich auch zur Datenverarbeitung.
In den Programmiersprachen Delphi und \Gls{r} werden zusätzlich automatische Verarbeitungsvorgänge durchgeführt.
Daten werden von extern und intern eingespeist.
Dabei handelt es sich um Punkte, Vektoren und Raster mit dazugehörigen Metadaten.
Im Rahmen der Datenverarbeitung werden Punktdaten interpoliert und mit Hilfe von Geostatistik Auswertungen aus originalen und verarbeiteten Daten erstellt.
Die Darstellung der Daten erfolgt über \Gls{umn} und \Gls{manifold}.
Als zentrales Element enthält die Datenbank neben den agrartechnischen Kennzahlen alle weiteren Daten des Unternehmens. In dieser Betrachtung werden einzig die agrartechnischen Kennzahlen berücksichtigt.
\begin{figure}[h]
\centering
\input{Abbildungen/Ist-Stand.tex}
\caption[Aktuelle Infrastruktur der Agri Con]{Aktuelle Infrastruktur der Agri Con}
\label{fig:iststand}
\end{figure}
%
\label{iststand-vorgaenge}

Es existiert eine Vielzahl von Vorgängen, welche zur Erhöhung der Useability in Hinsicht auf ihre Laufzeit verbessert werden können.
Für die Bewertung der Frameworks und schließlich zum Entwurf des Prototypen, ist es notwendig eine Auswahl der zu untersuchenden Vorgänge zu treffen.
Die Auswahl erfolgt durch Mitarbeiter der Agri~Con und hat Vorgänge mit der längsten Laufzeit als Ergebnis.
Dazu zählt das Laden von Daten zum Zwecke der Verarbeitung und Anzeige.
Die größten Datenmengen sind bei Punkten aus dem N-Sensor Bereich zu finden.
Die Anzeige der Punktdaten für einen Betrieb kann bis zu neun Sekunden dauern.
Punktdaten aus dem Docu Bereich werden ebenfalls nach mehreren Sekunden ausgegeben.
Die Punktdaten aus dem N-Sensor Bereich sind jedoch repräsentativ und werden deshalb betrachtet.
Weiterhin zählt Geostatistik zu diesen Vorgängen.
Punkte und Fahrspuren werden mit einem angepassten Kriging interpoliert und als Vektoren oder Raster abgespeichert.
Dies wird mit einer R Bibliothek realisiert und bei großen Datenmengen Nachts angestoßen.
Das spezielle Kriging im jeweiligen Framework zu implementieren ist aufwendig, weshalb ein unveränderter Kriging Algorithmus für den Vergleich verwendet werden muss, sofern R nicht verwendet werden kann.\footnote{Dieser spezielle kriging Algorithmus wurde im Auftrag durch Dritte erstellt.}
Diese zwei charakteristischen Vorgänge sind durch ein Framework zu realisieren.

\begin{figure}[h!]
\centering
\input{Abbildungen/Wunsch-Stand.tex}
\caption[Aufbau Wunsch-Stand]{\"{U}bersicht des Aufbaus des Wunsch-Standes bei Agri Con}
\label{fig:wunschstand}
\end{figure}
Das Ziel ist es, ein Framework zusätzlich in den Ist-Stand zu integrieren, wie in Abbildung \ref{fig:wunschstand} abgebildet.
Es soll dabei die aufwendigen Vorgänge durchführen, als Permanentspeicher für historische Daten dienen und Daten zur späteren Anzeige aufbereiten und bereitstellen.
Die aufwendigen Vorgänge werden in den Lasttests untersucht.
In welcher konkreten Form das Framework als Speicher für historische und aufbereitete Daten dient, ist abhängig vom Framework.
Auf Grund dessen ist dies nach Auswahl des Frameworks im Entwurf des Prototypen zu konkretisieren.

Für das Verständnis der Kapitel \ref{chapter:postgresxl} und \ref{chapter:tests} wird nachfolgend das Datenbankschema knapp beschrieben.
Es handelt sich um ein umfangreiches Schema, weshalb es aus mehreren Schemata besteht:
adwork, apps, bu, bucardo, catalogs, checks, common, demo, docu, farm, files, import, information\_{}schema, ips, krig, log, n, nutrients, ogeo, pg\_{}catalog, pp, public, rax, statistics, tasks, temp, topology, umn, utils und yield.
catalogs enthält statische Einträge analog eines globalen Kataloges.
common enthält Benutzerspezifische, farm dagegen Betriebsspezifische Daten.
Das Schema files beinhaltet Informationen zu Dateien, welche von Kunden auf den Server hochgeladen wurden sowie Metainformationen zu Dateiarten.
In den Schemata n und nutrients sind Daten zu N-Sensor zu finden.
Hilfsfunktionen und -tabellen des \Gls{umn} sind im Schema umn zu finden.
Alle Schemata beinhalten Funktionen, Trigger, Tabellen, Typen, Indizes und Contraints.
%- Grundschema der DB beschreieben (geografische agratechnische Daten und Daten des Unternehmens, der Organisation) -> ER mit dbvis?

%- quelldaten benennen und auf Anforderungen verweisen
%- Funktionsumfang hinsichtlich berechnung aufzeigen: was macht postgresql und was R
%- 


\section{Stand der Forschung}
\label{forschungsstand}
%TODO 4-5 Seiten
%Diskussion der Literatur
%grundlage für softwareauswahl: Ein Entscheidungsmodell für die Auswahl von Standardanwendungssoftware am Beispiel von Warenwirtschaftssystemen

Die jährlich stattfindende internationale Messe FOSSGIS\footnote{Industriemesse für freie und Open Source Software für Geoinformationssysteme \url{https://www.fossgis.de/}} zeigte in den Jahren 2012 und 2014, dass PostgreSQL mit PostGIS als freies \Gls{dbms} für \Gls{gis} eingesetzt wurde.
Zwar existiert eine Vielzahl von Elementen zur Darstellung und Kartenverwaltung% erzeugt aus den vorhandenen Daten
, aber es scheint keine geeignete Alternative zur Datenhaltung und Verarbeitung von komplexen räumlichen Daten zu geben.

Diese unternehmerische Sicht wird durch wissenschaftliche Arbeiten bestätigt.\\
So vergleicht Ahlers in seiner Bachelorarbeit \cite{ba:pgvsoracle} PostGIS mit Oracle Spatial, da auch für ihn PostGIS der größte freie Vertreter eines \Gls{gis} ist.
Brinkhoff listet in \cite[S.36]{book:prawirtinf} einzig PostGIS und MySQL als quelloffene \Gls{gis}, wobei MySQL eine geringe Funktionalität als \Gls{gis} besitzt.
Auf Seite 37 bringt er zum Ausdruck, dass sich die \Gls{gis} hinsichtlich Leistungsumfang und Syntax zum Teil deutlich von einander unterscheiden, was besonders im Vergleich von PostGIS und MySQL deutlich ist.
Thurm untersucht in \cite{ba:nosqlfuergeodaten} gängige NoSQL \Gls{dbms} für Eignung mit räumlichen Daten im Vergleich mit PostGIS und Oracle Spatial.
Darin kommt der Autor zu dem Schluss, dass es für einfache Datenobjekte in begrenzten Mengen nützlich ist die NoSQL Alternativen in Betracht zu ziehen.
Beispielsweise eignet sich die graphenbasierte Datenbank Neo4J für Operationen mit Strecken.
Für weniger spezielle und mehr komplexe Anforderungen sind laut Thurm einzig PostGIS und Oracle Spatial zu empfehlen.
Die NoSQL Systeme im Bereich der Datenverarbeitung von räumlichen Daten sind in den letzten Jahren entstanden und somit nicht ausgereift und umfangreich, so Thurm auf S.51.
%Übergang einbauen
\cite{ma:neo4j} vergleicht dagegen Neo4J mit PostGIS und kommt auch zu dem Schluss, dass graphenbasierte Datenhaltung zwar Vorteile besitzt, aber die Eignung individuell für den jeweiligen Anwendungsfall validiert werden muss.

Wissenschaftliche Dokumente in Form von wissenschaftlichen Artikeln sind zu diesem Thema vorhanden.
Diese beschreiben aber nur knapp Entwürfe eines Systems für einen speziellen Anwendungsfall, weshalb sie in diesem Zusammenhang nicht gebraucht werden können.
Der Artikel \cite{paper:hdfsspatial} beschreibt einen Entwurf zur Speicherung und Verarbeitung räumlicher Daten mit Hadoop, VegaGiStore genannt.
Der Forschungsbericht \cite{paper:spatialdistribution} erläutert dagegen Methoden zur Verteilung von Daten in einem verteilten System auf Grundlage der räumlichen Informationen.

Es existieren Arbeiten zu Randthemen und -betrachtungen sowie Teilproblemen, aber nicht zu Forschungszielen die Erkenntnisse zu freien Alternativen zu PostGIS und Eignung von verteilten \Gls{gis} für komplexe Anwendungsfälle liefern.

Neben wissenschaftlichen Dokumenten finden sich Informationen zu scheinbar geeigneten Systemen.
Das Fehlen von verlässlichen konkreten Informationen erschwert die Literaturrecherche.
Speziell Leistungsvergleiche sind in diesem Zusammenhang interessant, sind aber auf Grund fehlender Informationen nicht reproduzierbar.
Nachfolgend zwei Beispiele zu verschiedenen Systemen:\\
%Unternehmen werben mit höher Leistung als PostGIS:\\
GeoMesa: Zehn- bis 60-fache Antwortgeschwindigkeit bei GeoMesa gegenüber PostGIS, entsprechend der Abbildung \ref{fig:geomesaversuspostgis}.
\begin{figure}[hp]
\centering
\includegraphics[width=.8\textwidth]{Abbildungen/geomesa_versus_postgis.png}
\caption[Antwortzeiten von GeoMesa und PostGIS]{Antwortzeiten von GeoMesa und PostGIS, Quelle \cite[S.24]{website:slideshare:geomesa}}
\label{fig:geomesaversuspostgis}
\end{figure}

Postgres-XL: Bis zu sechsfache Antwortdauer von PostgreSQL gegenüber Postgres-XL in einem standardisierten TPC-H\footnote{TPC-H: \url{http://www.tpc.org/tpch/}} Benchmark bei Verwendung von vier DataNodes. Abbildung \ref{fig:pgxcversuspgsql} zeigt das Diagramm mit dem genannten Unterschied.
Die x-Achse ist nicht beschriftet, weswegen die Zuordnung der 15 Ergebnisse zu den 22 vorgegebenen Abfragen des TPC-H Benchmark nicht möglich ist.
Somit kann keine Bewertung der Laufzeiten erfolgen.
\begin{figure}
\centering
\includegraphics[width=.8\textwidth]{Abbildungen/postgresxl_versus_postgis.png}
\caption[TPC-H Benchmark von PostgreSQL und Postgres-XL]{TPC-H Benchmark von PostgreSQL und Postgres-XL, Quelle \cite[S.12]{website:slideshare:pgxc}}
\label{fig:pgxcversuspgsql}
\end{figure}

% Zusatz
%TODO stand und zukunft zu angeschnittenen Themen(frameworks, nosql,  BigData, ...) mit suchen (aus sicht des unternehmens - volkmar), auch bei publikationen der unis schaun
%GIS Frameworks; NoSQL (allgemein, gis bezug); BigData (allgemeine entwicklung, stand bei spatial data, allgemeine Methoden zur bewältigung); semantische räumliche daten; postgresql on steroids; technische entwicklung im precision farming
%TODO auch thema: semantische verknüpfung der geodaten übers web
Schnittthemen zur Arbeit sind ebenfalls von Interesse.
%BigData
Diese Arbeit bezieht sich auf umfangreiche strukturierte Daten.
Die Verwendung des Begriffes BigData ist in diesem Zusammenhang unzutreffend.
Nach Davenport ist BigData ein Sammelbegriff für umfangreiche unstrukturierte Daten, welche kontinuierlich anfallen.
Umfangreich meint dabei Größenordnungen, welche nicht mit einem Computer verwaltbar sind. (vgl. \cite[S.1]{book:bigdata})
Zwar fallen Daten bei Agri~Con kontinuierlich an, jedoch ist die gesamte Datenmenge geringer als ein Terabyte und wird Petybyte in absehbarer Zeit nicht erreichen.
Werden dagegen andere Unternehmen untersucht oder die Datenquellen der Agri~Con GmbH stark erweitert, sodass unstrukturierte Daten im Petabyte Bereich vorliegen, kann von BigData gesprochen werden.
Die Berücksichtigung dieses Begriffes führt zu anderen zu bewertenden Softwarequalitäten und somit zu anderen Ergebnissen.
Die Möglichkeiten der schemalosen Speicherung und Verarbeitung muss sich in diesem Fall in der gesamten Arbeit widerspiegeln.

%OGC
Untersuchungen im Kontext der räumlichen Datenverarbeitung profitieren von standardisierten Vorgehen und technischen Standards.
Die Standardisierung räumlicher Daten und Dienste wird durch das \Gls{ogc} durchgeführt, welches auf deren Website\footnote{Liste der Standards, veröffentlicht durch das \Gls{ogc}: \url{http://www.opengeospatial.org/standards/is}} 89 Standards auflistet.
Weit verbreitet ist Simple Features Access, womit räumliche Objekte allgemein für Implementierungen in beliebigen Programmiersprachen definiert werden.
Dieser Standard dient der Interoperabilität und direkten Implementierung.
Ergänzend dazu existiert die Filter Encoding Spezification, welche ein allgemeines Vorgehen zum Aufruf und zur Filterung von Datensätzen auf Programmierebene definiert.
Ziel ist dabei diese Definition einfach für beliebige Programmiersprachen validiert, umgewandelt und übersetzt werden kann.
Standards zu Webdiensten wie \Gls{wms}, \Gls{wfs}, \Gls{wps} und \Gls{wcs} für die Bereitstellung und Beschreibung räumlicher Daten sind auch häufig anzutreffen.
Zu nennen sind ebenso Formate für die Übertragung und Darstellung von räumlichen Daten wie \Gls{gml} und GeoTIFF.
Diese Standards kehren während der gesamten Untersuchung wieder, was den hohen Grad der Verbreitung betont.

%semantic
Bevor Standards vom \Gls{ogc} erstellt werden, wird die Notwendigkeit und der Umfang in der Forschung untersucht und diskutiert.
Die Formulierung und Verbreitung eines neuen Standards ist jeweils ein langwieriger Prozess.
Diese Prozesse dauern um so länger an, je mehr verschiedene Ansichten diskutiert werden müssen und wie viele Quasi-Standards bereits dazu existieren.
Dies wird bei der Standardisierung der semantischen Aufbereitung von Webdiensten mit räumlichen Datenkontext deutlich.
Die semantische Verwendung im Internet heißt Semantic Web.
Semantic Web zielt auf Verknüpfung von Wissen im Internet ab.
Dafür werden semantische Endpunkte pro Wissensbasis erstellt, die Endpunkte untereinander verknüpft und Daten in Tripeln, bestehend aus Subjekt, Prädikat und Objekt, im \Gls{rdf} Format gespeichert und übertragen.
Für Webdienste mit räumlichen Datenkontext existieren verschiedene Vorgehensweisen zur semantischen Aufbereitung\footnote{In der entsprechenden englischen Literatur wird statt spatial das Wort geospatial verwendet. Weiterhin wird eine Menge von zu verknüpfenden Webdiensten mit räumlichen Datenkontext als Spatial Data Infrastructures (SPI) zusammengefasst.}.
2009 wird bereits mit dem Paper \cite{report:semgeospatinte} gezeigt, dass Webdienste mit der Fähigkeit der Kartenauslieferung per \glqq{}getMap\grqq{} Anfrage semantisch aufbereitet und integriert werden können.
Ergänzend führen die Autoren Probleme und mögliche Lösungsansätze auf.
Im Paper \cite{report:semenspa} von 2010 stellen die Autoren ein Vorgehen zur semantischen Aufbereitung von OGC Diensten vor.
Für dieses Vorgehen bedienen sie sich vorhandener Untersuchungen und Werkzeuge.
%Auf Seite 16 wird das Vorgehen in drei Schritten zusammengefasst.
Der einzige Standard in Bezug auf Semantic Web des \Gls{ogc} ist GeoSPARQL\footnote{GeoSPARQL: \url{https://portal.opengeospatial.org/files/?artifact_id=47664}}.
Dieser erweitert die Abfragesprache \Gls{sparql} sowie das \Gls{rdf} Vokabular um räumliche Objekte.
Der Vorgang der semantischen Integration ist somit nicht standardisiert, jedoch die Werkzeuge und Rahmenbedingungen.
Zu den Rahmenbedingungen zählt, dass die semantische Darstellung von Koordinatenreferenzsystemen\footnote{Beispiel für eine semantische Darstellung: \url{http://www.w3.org/2003/01/geo/}} verwendet werden kann und verschiedene Versionen von \Gls{sparql} in den semantischen Datenbanken zum Einsatz kommen\footnote{Verschiedene Versionen von \Gls{sparql} enthalten unterschiedliche Objekte und Möglichkeiten zur räumlichen Modellierung.} (vgl. \cite[S.5]{book:semgeosparql}).
Für zukünftig relevante Standards, Methoden und Werkzeuge sind die Ergebnisse der Spatial Data on the Web Working Group (SDWWG)\footnote{Spatial Data on the Web Working Group (SDWWG): \url{https://www.w3.org/community/geosemweb/}} und der Geospatial Semantic Web Community Group (GSWCG)\footnote{Geospatial Semantic Web Community Group (GSWCG): \url{http://www.opengeospatial.org/projects/groups/sdwwg}} zu beobachten. 

%postgresql on steroids
Ein weiteres Thema ist die Steigerung der Leistungsfähigkeit von PostgreSQL.
Dabei werden in der Regel die Dokumentation von PostgreSQL und Ratschläge der Community als Handlungsempfehlungen herangezogen.
Im Zuge der großen Verbreitung von PostgreSQL erschien Literatur, welche diese Handlungsempfehlungen validierend aufbereitete und ergänzt veröffentlichte.
Auf Grund kontinuierlich Verändernder Hardware und steigender Versionsnummern, ist diese Literatur trotzdem mit aktuellen Informationen aus der Community zu verwenden.
Zur Steigerung der Leistungsfähigkeit sind \cite{book:postgresqladmin} und \cite{book:postgresqlperformance} relevante Bücher.
%
Eisentraut und Helmle gehen vorrangig auf PostgreSQL interne Veränderungen ein, dazu zählt:
Szenariobasierte Verwendung von Indizes, Query Planer Konfiguration, Verwendung von Fremdschlüsseln, Handhabung von Verbindungen zu und zwischen PostgreSQL mit Einstellungsparametern und Tools sowie Clustering und Replikation. (vgl. \cite[S.206 ff.]{book:postgresqladmin})
%
Smith geht tief auf Änderungen und Verwendung der Hardware, interner Möglichkeiten und vorhandener Tools ein.
So werden konkrete Hardwarekomponenten beleuchtet und die Messbarkeit dieser vorgestellt.
Die Ausführungen beziehen sich dabei auch auf plattformspezifische Eigenheiten.
Schwerpunkte sind routinemäßige Arbeiten und Erläuterungen sowie Verbesserungen zu SQL Querys.
Neben der Menge an Vorschlägen und Erfahrungswerten des Autors, weist dieser darauf hin, dass Probleme mit der Leistungsfähigkeit mit der Anwendung zusammenhängen und jedes Problem individuell gelöst werden muss (vgl. \cite[S.18]{book:postgresqlperformance}).
Die von Smith vorgestellten Methoden zu Zwischenspeicherung, Konfiguration, Benchmarking, Indexierung, SQL Query Veränderung, Datenbankstatistiken, Monitoring, Pooling und Partitionierung werden von ihm als allgemeine Lösungsmethoden dargelegt, sodass die Durchführung dieser Methoden den Großteil der Probleme behebt und ihnen vorbeugt.



