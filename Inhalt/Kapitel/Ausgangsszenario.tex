\chapter{Ausgangsszenario}
\label{chapter:ausgangsszenario}

\section{Anforderungen}
\label{Anforderungen}

% kartografisches Produkt
Aktuelle Möglichkeiten der Datenerfassung über Sensoren und moderne Probenahmegeräte führen zu mehr und mehr Datensätzen, die für einen Landwirtschaftsbetrieb ausgewertet werden müssen. Darüber hinaus besteht die Notwendigkeit, Daten Jahresübergreifend und betriebsübergreifend auszuwerten, um pflanzenbauliche Zusammenhänge über statistische Methoden untersuchen zu können.
In den letzten 3 Jahren wurde beispielsweise nur zum Thema N-Versorgung\footnote{Stickstoffdüngung und -aufnahme} für einen Betrieb etwa 800 Datensätze mit 1,9 Mio Einträgen erfasst. Alle diese Daten haben einen räumlichen Bezug, sie müssen weiterverarbeitet, kartographisch aufbereitet und dargestellt werden.\\
Daraus ergeben sich verschiedenen Anforderungen an die Technologie, die für die Verarbeitung, Analyse und Darstellung verwendet wird:
\begin{itemize}
\item PostgreSQL mit PostGIS zum Datenimport und -export nutzbar
\item Gruppieren und Filtern mit geringer Laufzeit
\item parallele Berechnung\footnote{hier Geostatistik} über große Datenmengen mit geringer Laufzeit
\item Räumliche Berechnungen wie Verschneiden und Overlays
%\item  Unterschiedliche Prinzipien der Kartengenerierung, hier dynamisches rendern aus dem Datenbestand zur Laufzeit oder dynamisches rendern bei Dateneingang wodurch vorgerenderte Karten bereitstehen % caches wirklich mit untersuchen? wenn ja in Schnittstellen aufnehmen
\item nutzbare Schnittstelle zur Darstellung mit dem \Gls{umn}
\end{itemize}

% TODO: ergänzen, dass Anforderungen von Ist-Stand und möglichen Verbesserungen herrühren

% Eventuell technische Sicht extra darstellen, um Eignung der Systeme besser herausarbeiten zu können

Konkret handelt es sich bei den Eingangsdaten um folgende:
\begin{description}
\item[Pflanzenbauliche Daten]\footnote{Sensoren, Bodenuntersuchung, \Gls{bonitur}, Logger} Punktdaten
\item[Basisdaten wie Feldgrenzen] Vektordaten
\item[Externe Satelliteninformationen und Multispektralanalysen] Rasterdaten
\end{description}

\subsection{Softwarequalität}
\label{softwarequalität}
%TODO: warum hier die qualitätsmerkmale
Qualitätsmerkmale sind nach DIN 9126\footnote{DIN 9126 wurde durch ISO/IEC 25000 ersetzt, jedoch sind beide nur proprietär verfügbar} in \cite[S.258 f.]{book:lehrbuchsoftware} Funktionalität, Zuverlässigkeit, Benutzbarkeit, Effizienz, Änderbarkeit und Übertragbarkeit.
Diese Merkmale werden durch Qualitätskriterien für jeden Anwendungsfall konkretisiert.
Nachfolgend werden die Qualitätsmerkmale für diesen Anwendungsfall dargestellt und darauf die zu untersuchenden aufgelistet.
Da die zu analysierenden Systeme eine Datenbank beinhalten, welche mit räumlichen Datentypen arbeitet, wurde die im Anhang C von \cite{book:objdbs} enthaltene Checkliste zur Auswahl eines \Gls{odbms} berücksichtigt.

\textbf{Funktionalität}\\
Das System stellt alle geforderten Funktionen mit den definierten Eigenschaften zur Verfügung.
\begin{itemize}
\item \textbf{Richtigkeit:} Ergebnisse sind korrekt oder ausreichend genau.
Die originalen räumlichen Daten werden von Sensoren erfasst, in wessen Toleranzbereich die Ergebnisse der Verarbeitung durch das Framework liegen müssen.
\item \textbf{Interoperabilität:} Es sind Schnittstellen zur Ein- und Ausgabe vorhanden. Dabei soll es sich um PostgreSQL Import sowie PostgreSQL und \Gls{umn} Export handeln.
\item \textbf{Funktionsumfang:} Mindestens die benannte und essentielle Menge an Funktionalitäten wird bereitgestellt. Dazu zählt:
parallele Verarbeitung, Gruppierungs-, Filter-, Verschneidungs- sowie Overlayfunktionen, Geostatistik und Umrechnung zwischen Koordinatensystemen und -formaten. Außerdem sind vorhandene Datentypen und Schemaversionierung von Interesse.
\item \textbf{Ordnungsmäßigkeit:} Die Implementation des Systems und dessen Funktionen erfüllt Normen, Vereinbarungen, gesetzliche Bestimmungen und andere Vorschriften. Hierzu ist zu nennen, dass besonders Berechnungsfunktionen nach mathematischen Gesetzen implementiert sein müssen. Konkret sind Berechnungen der räumlichen Verarbeitung nach anerkannten definierten Algorithmen durchzuführen. Weiterhin sind Definitionen der Koordinatenreferenzsysteme\footnote{siehe \Glspl{epsg-code}}, die mathematisch korrekte deterministische sowie stochastische Interpolation und Interpolation nach Krige einzuhalten. Auch Allgemeine Anforderungen an \Gls{dbms} wie Integritätssicherung, Datensicherheit, Mehrbenutzerbetrieb, Datenunabhängigkeit und Zugangssicherung müssen erfüllt werden.
\end{itemize}
\ \\
%
\textbf{Zuverlässigkeit}\\
Nach \cite[S.259]{book:lehrbuchsoftware} wird Zuverlässigkeit als die Fähigkeit einer Software ihr Leistungsniveau unter festgelegten Bedingungen über einen festgelegten Zeitraum bewahren definiert.
%Nutzung von Tools zur Überwachung und Konfiguration immanent.
\begin{itemize}
\item \textbf{Fehlertoleranz:} Das System sollte auftretende Fehler des Tagesgeschäftes abfangen und weiterarbeiten. Besonders Fehler in den Quelldaten können zu Fehlern während der Ausführung von Berechnungen führen, was per s\'{e} abgefangen werden muss.
\item \textbf{Wiederherstellbarkeit:} Auch die Möglichkeit bei einem schwerwiegendem den Zustand der abgebrochenen Operationen wiederherzustellen ist ein zu betrachtendes Qualitätskriterium.
\item \textbf{\Gls{mttf}:} Diese statische Kenngröße der erfahrungsgemäßen mittleren Lebensdauer ist für kritische Systeme relevant.
\end{itemize}
\ \\
%
\textbf{Benutzbarkeit}\\
Qualität des Zugangs für Benutzer sowie Eignung für eine oder mehrere Benutzergruppen.
\begin{itemize}
\item \textbf{Verständlichkeit} 
\item \textbf{Bedienbarkeit}
\item \textbf{Dokumentation:} Eine ausführliche, aktuelle und korrekte Dokumentation ist Voraussetzung zur produktiven Verwendung.
\item \textbf{Eignung:} Die angestrebte Benutzergruppe muss mit der aktuellen Benutzergruppe übereinstimmen. Die aktuelle Benutzergruppe ist Programmierer bzw. Administrator.
\end{itemize}
\ \\
%
\textbf{Effizienz}\\
Effizienz ist das Verhältnis zwischen Auslastung der Hardware und erfolgreich bearbeiteten Aufgaben. Nach \cite[S.21]{book:Leistungsanalyse} ist Leistung paralleler Programme das Verhältnis des Speedups zur Anzahl der verwendeten Prozessoren. Wobei Speedup als Verhältnis der Ausführungszeiten zwischen der auf N Prozessoren ausgeführten parallelen Version eines Programms und der sequentiellen Version des Programmes definiert ist. Diese Definitionen treffen für die zu untersuchenden Systeme zu, da es sich um parallelisierende \Gls{gis} handelt.
\begin{itemize}
\item \textbf{Zeitverhalten:} Oder auch Laufzeitverhalten genannt, dient allgemein zur Darstellung des Durchsatzes. Die Skalierung des Systems zählt hier dazu. Dies wird speziell durch zusätzliche Leistungstests beurteilt.
\item \textbf{Verbrauchsverhalten:} Das Verhältnis aus erbrachter Leistung und dem dafür notwendig gewesenen Aufwand in Form von Hardwarenutzung.
\item \textbf{Skalierbarkeit:} Anzahl der zu verwendenden Computer um nach dem Speedup eine Effizienzsteigerung im Gegensatz zum Einsatz bei einem Computer zu erreichen.
\end{itemize}
\ \\
%
\textbf{Änderbarkeit}\\
Aufwand zur Verbesserung oder Anpassung der Umgebung und der Spezifikationen, auch Wartungsaufwand genannt.
\begin{itemize}
\item \textbf{Analysierbarkeit:} \glqq Aufwand, um Mängel oder Ursachen von Versagen zu diagnostizieren oder um änderungsbedüftige Teile zu bestimmen.\grqq\ \cite[S. 260]{book:lehrbuchsoftware}
\item \textbf{Modifizierbarkeit:} Notwendiger Aufwand für Änderungen zum Ziele der Verbesserung und Fehlerbehebung.
\item \textbf{Stabilität:} Wahrscheinlichkeit von ungewollten Auswirkungen durch Änderungen.
\item \textbf{Prüfbarkeit:} Oder Testbarkeit als Merkmal, welches die Möglichkeiten und den Aufwand zum testen der originalen und geänderten Systeme darstellt.
\end{itemize}
\ \\
%
\textbf{Übertragbarkeit}\\
Die Fähigkeit das System auf andere Hard- und Software sowie andere Vorgehensweisen zu migrieren.
\begin{itemize}
\item \textbf{Anpassbarkeit:} Möglichkeiten des unveränderten Systems Änderungen vorzunehmen.
\item \textbf{Installierbarkeit:} Systemvoraussetzung und Aufwand zur Installation des Systems.
\end{itemize}
\ \\
%
\textbf{Nichttechnische Kriterien}\\
Erweiterte Qualitätskriterien, welche nicht nach der DIN 9126 zugeordnet werden können.
\begin{itemize}
\item \textbf{Herstellerfirma und Produkt:} Dazu zählt die Marktposition, der Preis, die Produktplanung und der Service.
\end{itemize}
\ \\
%
Die zu untersuchenden Qualitätskriterien für die Softwareauswahl sind Funktionsumfang, Fehlertoleranz, Dokumentation, Zeitverhalten, Analysier- und Modifizierbarkeit.


\subsection{Qualitätsmetriken}
\label{qualitätsmetriken}
%TODO: essentielles markieren oder stärker wichten
Zu den wichtigen Qualitätsmerkmalen aus Kapitel \ref{softwarequalität} sind folgend Kriterien definiert, sowie je eine Bewertungsvorschrift und die geforderte Mindestbewertung für den Anwendungsfall angegeben.

\textbf{Richtigkeit:}\\
Berechnungen sind zu 100\% korrekt. Ausnahme ist dabei die Berechnung von Koordinaten. Dabei haben die Ergebnisse bis acht Stellen nach dem Komma korrekt zu sein.
Die statische Abbildung ist dabei $[korrekt, nicht\ korrekt]\ nach\ [1, 0]$ und die geforderte Mindestbewertung 1.

\textbf{Interoperabilität:}\\
Ist der Import und Export von räumlichen Daten aus PostgreSQL sowie eine Anbindungsmöglichkeit an den \Gls{umn}.
Statische Abbildung:\\
$[Datenschnittstelle\ und\ UMN\ Schnittstelle\ vorhanden,Datenschnittstelle\ vorhanden,$\\$UMN\ Schnittstelle\ vorhanden,keine\ Schnittstelle\ vorhanden]\ nach\ [12,7,5,0]$\\
Der Bereich bis 12 soll die Wichtigkeit des Vorhandenseins der Schnittstellen verdeutlichen.
Die geforderte Mindestbewertung ist 7.

\textbf{Funktionsumfang:}\\
Tabelle \ref{table:funktionsumfang} gibt die Wertung bei Existenz der einzelnen Funktionen wieder.
Existiert die Funktion nicht, ist die Wertung Null.
Ein Zwischenwert bei eingeschränkter Funktionalität ist möglich.
Maximale Wertung: 61 %\FPtrunc\Gesamtsumme\Gesamtsumme{0}\FPprint\Gesamtsumme\\
Es müssen mindestens geografische Datentypen, Filterfunktionen und parallele Verarbeitung vorhanden sein.
\begin{table}[h]
\centering
\begin{tabular}{l|c}
\textbf{Funktion} & \textbf{Wertung} \\ \hline
parallele Verarbeitung & \psum{2} \\ \hline
geografische Datentypen & \psum{14} \\ \hline
Umrechnung zwischen Koordinatensystemen & \psum{10} \\ \hline
Gruppierungsfunktionen & \psum{10} \\ \hline
Verschneidungsfunktionen & \psum{4} \\ \hline
Overlayfunktionen & \psum{4} \\ \hline
Geostatistik & \psum{6} \\ \hline
Filterfunktionen & \psum{10} \\ \hline
Schemaversionierung & \psum{1}
\end{tabular}
\caption{Wertungstabelle Funktionsumfang}
\label{table:funktionsumfang}
\end{table}

\textbf{Fehlertoleranz:}\\
Es gilt zu messen, ob Fehler bei einer Berechnung andere verschränkt gleichzeitig laufende Berechnungen beeinträchtigen.
Aus diesem Grund wird Unabhängigkeit auf eins und Abhängigkeit auf null abgebildet.
Die geforderte Mindestbewertung ist 1.

\textbf{Dokumentation:}\\
Vorhandene Dokumentation ist nach einzelnen Themen zu bewerten.
Dabei kann ein maximaler Wert von 13 erreicht werden.
\begin{table}[h]
\centering
\begin{tabular}{l|l}
\textbf{Dokumentation zu} & \textbf{Wertung je Eintrag} \\ \hline
Installation, Zeitverhalten & 1 \\ \hline
Funktionsumfang & 2 \\ \hline
Interoperabilität, Best practise, Anpassbarkeit & 3 
\end{tabular}
\caption{Wertungstabelle Dokumentation}
\label{table:dokumentation}
\end{table}
Die geforderte Mindestbewertung ist 6.

\textbf{Zeitverhalten:}\\
%TODO: Werte aus Ist-Stand in Bezug auf Lasttests verwenden
Konkret wird eine Beschleunigung aufwendiger Vorgänge angestrebt.
Die statische Abbildung auf Bezug auf die Laufzeiten des Ist-Standes:\\
$[Zeit\ wird\ überschritten,\ Zeit\ ist\ gleich,\ Zeit\ wird\ unterschritten]\ nach\ [0,1,3]$\\
Der Abruf von Punktdaten dauert neun Sekunden und die Interpolation ...
%Verweis auf Anhang

\textbf{Modifizierbarkeit:}\\
Anpassungen des Frameworks hinsichtlich der folgenden Punkte erhöhen den Wert um eins:\\
Verwendung eigener Datentypen, Erstellung eigener Schnittstellen, Erstellung eigener Funktionen, Verwendung der Programmiersprachen Scala oder R, Anlegen eigener Berechnungsvorgängen zur späteren Abarbeitung\\
Die geforderte Mindestbewertung ist abhängig vom Funktionsumfang und der Interoperabilität.
Jedoch sollten fehlende Funktionen und Schnittstellen erstellt werden können.

\subsection{Testfälle}
Zur definierten und wiederholbaren Erfassung der Erfüllung von speziellen Kriterien wie Funktionalität und Zeitverhalten,
folgt die Definition der Funktions- und Lasttests.

\subsubsection{Funktionstests}
Nach der Definition in \ref{grundlagen-funktionstests} werden hiermit spezielle Funktionalitäten auf Vorhandensein und die Menge der Schnittstellen sowie der Austauschformate getestet.
Diese Funktionstest sind nicht automatisiert für unterschiedliche Frameworks durchführbar.
Deshalb werden sie manuell anhand der Spezifikation und des Quellcodes durchgeführt und die Ergebnisse tabellarisch festgehalten.

Folgende Schnittstellen sind zu testen:
\begin{itemize}
\item PostgreSQL
\item PostGIS
\item \Gls{umn}
\end{itemize}

Als Austauschformate ist mindestens eines der folgenden Datentypen zum Datenaustausch notwendig:
\begin{itemize}
\item Simple Feature Access
\item Objekte der JTS
\item PostGIS geometry
\end{itemize}

Speziell in GIS gilt es die Koordinatenreferenzsysteme zu analysieren.
Die EPSG Codes 3578 und 4326 werden im Anwendungsfall verwendet.

Für folgende Aufgaben sind die Funktionen zu analysieren:
\begin{itemize}
\setlength{\itemsep}{-8pt}
\item Umwandlung zwischen Koordinatensystemen
\item Verschneidung von räumlichen Daten
\item Geostatistik
\item Interpolation
\item Kriging
\item topologische Filterung
\item räumliche Filterung
\end{itemize}
Dabei ist stets zu berücksichtigen mit welchen Datentypen die Funktionen verwendet werden können.

%- ist als Datenquelle/Import PostgreSQL angebbar und Daten sind importierbar? Welche nicht?
%- existieren räumliche Datentypen? eventuell unter anderem namen, mit anderen attributen/funktionen. Unterschied aufzeigen
%- Welche Koordinatenreferenzsysteme können verwendet werden? Kann man verschiedene importieren/exportieren?
%- gibt es group funktionen? bei welchen Datentypen geht das?
%- Gibt es Funktionen zum Verschneiden von bsw. Polygonen mit anderen?
%- Können Daten übereinander gelegt und ausgewertet werden?
%- Gibt es funktionen zur geostatistik? Sind die Grundfunktionen für Kriging vorhanden?
%- können räumliche Daten geografisch/topologisch gefiltert werden?

\subsubsection{Lasttests}
Die Basis für die Erstellung von Lasttests ist Kapitel \ref{iststand-vorgaenge}.
Ein Lasttest misst die Laufzeit zur Aggregation von Punktdaten.
Dies ist auf dem System der Datenbank bzw. des Masters über den gängigen Weg, vorrangig SQL, durchzuführen.
Die Laufzeitmessung der Anfragen erfolgt über diese mit Parametern der Anfrage.
% select * from n.nsensorlogs where farmid=1038
Analog ist ein weiterer Lasttest zur Interpolation bzw. Kriging durchzuführen.
%funktion ist noch zu erstellen

%Lasttests zur Persistierung von Eingangsdaten?

\section{Ist-Stand}
\label{IstStand}
%groben Ablauf textuell und grafisch darstellen
% geplanten Einsatz des Prototypen ebenso darstellen 

Die Durchführung einer Softwareauswahl zum teilweisen Ersatz eines bestehenden Systems setzt die Analyse des Systems voraus.
In diesem Unterkapitel wird der Ist-Stand sowie der der angestrebte Zustand nach Einbau des Prototypen dargestellt.
Firmeninterne Schnittstellen mit dem ist-Stand werden nicht konkretisiert, da sie den Anwendungsfall nicht schneiden.

In Abbildung \ref{fig:iststand} ist die Übersicht des Aufbaus ersichtlich.
Das Herzstück bildet die objektrelationale Datenbank PostgreSQL mit der geografischen Erweiterung PostGIS.
Diese dient zur Datenhaltung und wesentlich auch zur Datenverarbeitung.
In den Programmiersprachen Delphi und \Gls{r} werden zusätzlich automatische Verarbeitungsvorgänge durchgeführt.
Daten werden von extern und intern eingespeist.
Dabei handelt es sich um Punkte, Vektoren und Raster mit dazugehörigen Metadaten.
Im Rahmen der Datenverarbeitung werden Punktdaten interpoliert und mit Hilfe von Geostatistik Auswertungen aus originalen und verarbeiteten Daten erstellt.
Als zentrales Element enthält die Datenbank neben den agrartechnischen Kennzahlen alle weiteren Daten des Unternehmens. In dieser Betrachtung werden einzig die agrartechnischen Kennzahlen berücksichtigt.
\begin{figure}[h]
\centering
\input{Abbildungen/Ist-Stand.tex}
\caption[Aktuelle Infrastruktur bei Agri Con]{Aktuelle Infrastruktur bei Agri Con}
\label{fig:iststand}
\end{figure}
%
\label{iststand-vorgaenge}

Es existiert eine Vielzahl von Vorgängen, welche zur Erhöhung der Useability in Hinsicht auf ihre Laufzeit verbessert werden können.
Für die Bewertung der Frameworks und schließlich zum Entwurf des Prototypen, ist es notwendig eine Auswahl der zu untersuchenden Vorgängen zu treffen.
Die Auswahl erfolgt durch Mitarbeiter der Agri Con und hat Vorgänge mit der längsten Laufzeit als Ergebnis.
Dazu zählt das Laden von Daten zum Zwecke der Verarbeitung und Anzeige.
Die größten Datenmengen sind bei Punkten aus dem N-Sensor Bereich zu finden.
Die Anzeige der Punktdaten für einen Betrieb kann bis zu neun Sekunden dauern.
Punktdaten aus dem Docu Bereich werden ebenfalls nach mehrere Sekunden ausgegeben.
Die Punktdaten aus dem N-Sensor Bereich sind jedoch repräsentativ und werden deshalb betrachtet.
Weiterhin zählt Interpolation zu diesen Vorgängen.
Punkte und Fahrspuren werden mit einem angepassten Kriging interpoliert und als Vektoren und Raster abgespeichert.
Dies wird mit einer R Bibliothek realisiert und bei großen Datenmengen Nachts angestoßen.
Das spezielle Kriging im jeweiligen Framework zu implmentieren ist aufwendig, weshalb ein unveränderter Kriging Algorithmus für den Vergleich verwendet werden muss.
Diese zwei charakteristischen Vorgänge sind durch ein Framework zu realisieren.


\begin{figure}[h]
\centering
\input{Abbildungen/Wunsch-Stand.tex}
\caption[Aufbau Wunsch-Stand]{\"{U}bersicht des Aufbaus des Wunsch-Standes}
\label{fig:wunschstand}
\end{figure}
Das Ziel ist es, ein Framework zusätzlich in den Ist-Stand zu integrieren.
Es soll dabei die aufwendigen Vorgänge durchführen, als Permanentspeicher für historische Daten dienen und Daten zur späteren Anzeige aufbereiten und bereitstellen.
Die aufwendigen Vorgänge werden in den Lasttests untersucht.
In welcher konkreten Form das Framework als Speicher für historische und aufbereitete Daten dient, ist abhängig vom Framework.
Auf Grund dessen ist dies nach Auswahl des Frameworks im Entwurf des Prototypen zu konkretisieren.

%- quelldaten benennen und auf Anforderungen verweisen
%- Grundschema der DB beschreieben (geografische agratechnische Daten und Daten des Unternehmens, der Organisation)
%- Funktionsumfang hinsichtlich berechnung aufzeigen: was macht postgresql und was R
%- 
