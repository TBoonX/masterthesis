\chapter{Ausgangsszenario}
\label{chapter:ausgangsszenario}

\section{Anforderungen}
\label{Anforderungen}

% kartografisches Produkt
Aktuelle Möglichkeiten der Datenerfassung über Sensoren und moderne Probenahmegeräte führen zu mehr und mehr Datensätzen, die für einen Landwirtschaftsbetrieb ausgewertet werden müssen. Darüber hinaus besteht die Notwendigkeit, Daten Jahresübergreifend und betriebsübergreifend auszuwerten, um pflanzenbauliche Zusammenhänge über statistische Methoden untersuchen zu können.
In den letzten 3 Jahren wurde beispielsweise nur zum Thema N-Versorgung\footnote{Stickstoffdüngung und -aufnahme} für einen Betrieb etwa 800 Datensätze mit 1,9 Mio Einträgen erfasst. Alle diese Daten haben einen räumlichen Bezug, sie müssen weiterverarbeitet, kartographisch aufbereitet und dargestellt werden.\\
Daraus ergeben sich verschiedenen Anforderungen an die Technologie, die für die Verarbeitung, Analyse und Darstellung verwendet wird:
\begin{itemize}
\item PostgreSQL mit PostGIS zum Datenimport und -export nutzbar
\item Gruppieren und Filtern mit geringer Laufzeit
\item parallele Berechnung\footnote{hier Geostatistik} über große Datenmengen mit geringer Laufzeit
\item Räumliche Berechnungen wie Verschneiden und Overlays
%\item  Unterschiedliche Prinzipien der Kartengenerierung, hier dynamisches rendern aus dem Datenbestand zur Laufzeit oder dynamisches rendern bei Dateneingang wodurch vorgerenderte Karten bereitstehen % caches wirklich mit untersuchen? wenn ja in Schnittstellen aufnehmen
\item nutzbare Schnittstelle zur Darstellung mit dem \Gls{umn}
\end{itemize}

% TODO: ergänzen, dass Anforderungen von Ist-Stand und möglichen Verbesserungen herrühren

% Eventuell technische Sicht extra darstellen, um Eignung der Systeme besser herausarbeiten zu können
\newpage
Konkret handelt es sich bei den Eingangsdaten um folgende:
\begin{description}
\item[Pflanzenbauliche Daten]\footnote{Sensoren, Bodenuntersuchung, \Gls{bonitur}, Logger} Punktdaten
\item[Basisdaten wie Feldgrenzen sowie Interpolationen]\footnote{Interpolationen als Ergebnis von Nährstoffverteilung und Bodenscanner} Vektordaten
\item[Externe Satelliteninformationen und Multispektralanalysen] Rasterdaten
\end{description}

\subsection{Softwarequalität}
\label{softwarequalität}
%TODO: warum hier die qualitätsmerkmale
Qualitätsmerkmale sind nach DIN 9126\footnote{DIN 9126 wurde durch ISO/IEC 25000 ersetzt, jedoch sind beide nur proprietär verfügbar} in \cite[S.258 f.]{book:lehrbuchsoftware} Funktionalität, Zuverlässigkeit, Benutzbarkeit, Effizienz, Änderbarkeit und Übertragbarkeit.
Diese Merkmale werden durch Qualitätskriterien für jeden Anwendungsfall konkretisiert.
Nachfolgend werden die Qualitätsmerkmale für diesen Anwendungsfall dargestellt und darauf die zu untersuchenden aufgelistet.
Da die zu analysierenden Systeme eine Datenbank beinhalten, welche mit räumlichen Datentypen arbeitet, wurde die im Anhang C von \cite{book:objdbs} enthaltene Checkliste zur Auswahl eines \Gls{odbms} berücksichtigt.

\textbf{Funktionalität}\\
Das System stellt alle geforderten Funktionen mit den definierten Eigenschaften zur Verfügung.
\begin{itemize}
\item \textbf{Richtigkeit:} Ergebnisse sind korrekt oder ausreichend genau.
Die originalen räumlichen Daten werden von Sensoren erfasst, in wessen Toleranzbereich die Ergebnisse der Verarbeitung durch das Framework liegen müssen.
\item \textbf{Interoperabilität:} Es sind Schnittstellen zur Ein- und Ausgabe vorhanden. Dabei soll es sich um PostgreSQL Import sowie PostgreSQL und \Gls{umn} Export handeln.
\item \textbf{Funktionsumfang:} Mindestens die benannte und essentielle Menge an Funktionalitäten wird bereitgestellt. Dazu zählt:
parallele Verarbeitung, Gruppierungs-, Filter-, Verschneidungs- sowie Overlayfunktionen, Geostatistik und Umrechnung zwischen Koordinatensystemen und -formaten. Außerdem sind vorhandene Datentypen und Schemaversionierung von Interesse.
\item \textbf{Ordnungsmäßigkeit:} Die Implementation des Systems und dessen Funktionen erfüllt Normen, Vereinbarungen, gesetzliche Bestimmungen und andere Vorschriften. Hierzu ist zu nennen, dass besonders Berechnungsfunktionen nach mathematischen Gesetzen implementiert sein müssen. Konkret sind Berechnungen der räumlichen Verarbeitung nach anerkannten definierten Algorithmen durchzuführen. Weiterhin sind Definitionen der Koordinatenreferenzsysteme\footnote{siehe \Glspl{epsg-code}}, die mathematisch korrekte deterministische sowie stochastische Interpolation und Interpolation nach Krige einzuhalten. Auch Allgemeine Anforderungen an \Gls{dbms} wie Integritätssicherung, Datensicherheit, Mehrbenutzerbetrieb, Datenunabhängigkeit und Zugangssicherung müssen erfüllt werden.
\end{itemize}
\ \\
%
\textbf{Zuverlässigkeit}\\
Nach \cite[S.259]{book:lehrbuchsoftware} wird Zuverlässigkeit als die Fähigkeit einer Software ihr Leistungsniveau unter festgelegten Bedingungen über einen festgelegten Zeitraum bewahren definiert.
%Nutzung von Tools zur Überwachung und Konfiguration immanent.
\begin{itemize}
\item \textbf{Fehlertoleranz:} Das System sollte auftretende Fehler des Tagesgeschäftes abfangen und weiterarbeiten. Besonders Fehler in den Quelldaten können zu Fehlern während der Ausführung von Berechnungen führen, was per s\'{e} abgefangen werden muss.
\item \textbf{Wiederherstellbarkeit:} Auch die Möglichkeit bei einem schwerwiegendem den Zustand der abgebrochenen Operationen wiederherzustellen ist ein zu betrachtendes Qualitätskriterium.
\item \textbf{\Gls{mttf}:} Diese statische Kenngröße der erfahrungsgemäßen mittleren Lebensdauer ist für kritische Systeme relevant.
\end{itemize}
\ \\
%
\textbf{Benutzbarkeit}\\
Qualität des Zugangs für Benutzer sowie Eignung für eine oder mehrere Benutzergruppen.
\begin{itemize}
\item \textbf{Verständlichkeit} 
\item \textbf{Bedienbarkeit}
\item \textbf{Dokumentation:} Eine ausführliche, aktuelle und korrekte Dokumentation ist Voraussetzung zur produktiven Verwendung.
\item \textbf{Eignung:} Die angestrebte Benutzergruppe muss mit der aktuellen Benutzergruppe übereinstimmen. Die aktuelle Benutzergruppe ist Programmierer bzw. Administrator.
\end{itemize}
\ \\
%
\textbf{Effizienz}\\
Effizienz ist das Verhältnis zwischen Auslastung der Hardware und erfolgreich bearbeiteten Aufgaben. Nach \cite[S.21]{book:Leistungsanalyse} ist Leistung paralleler Programme das Verhältnis des Speedups zur Anzahl der verwendeten Prozessoren. Wobei Speedup als Verhältnis der Ausführungszeiten zwischen der auf N Prozessoren ausgeführten parallelen Version eines Programms und der sequentiellen Version des Programmes definiert ist. Diese Definitionen treffen für die zu untersuchenden Systeme zu, da es sich um parallelisierende \Gls{gis} handelt.
\begin{itemize}
\item \textbf{Zeitverhalten:} Oder auch Laufzeitverhalten genannt, dient allgemein zur Darstellung des Durchsatzes. Die Skalierung des Systems zählt hier dazu. Dies wird speziell durch zusätzliche Leistungstests beurteilt.
\item \textbf{Verbrauchsverhalten:} Das Verhältnis aus erbrachter Leistung und dem dafür notwendig gewesenen Aufwand in Form von Hardwarenutzung.
\item \textbf{Skalierbarkeit:} Anzahl der zu verwendenden Computer um nach dem Speedup eine Effizienzsteigerung im Gegensatz zum Einsatz bei einem Computer zu erreichen.
\end{itemize}
\ \\
%
\textbf{Änderbarkeit}\\
Aufwand zur Verbesserung oder Anpassung der Umgebung und der Spezifikationen, auch Wartungsaufwand genannt.
\begin{itemize}
\item \textbf{Analysierbarkeit:} \glqq Aufwand, um Mängel oder Ursachen von Versagen zu diagnostizieren oder um änderungsbedüftige Teile zu bestimmen.\grqq\ \cite[S. 260]{book:lehrbuchsoftware}
\item \textbf{Modifizierbarkeit:} Notwendiger Aufwand für Änderungen zum Ziele der Verbesserung und Fehlerbehebung.
\item \textbf{Stabilität:} Wahrscheinlichkeit von ungewollten Auswirkungen durch Änderungen.
\item \textbf{Prüfbarkeit:} Oder Testbarkeit als Merkmal, welches die Möglichkeiten und den Aufwand zum testen der originalen und geänderten Systeme darstellt.
\end{itemize}
\newpage
\textbf{Übertragbarkeit}\\
Die Fähigkeit das System auf andere Hard- und Software sowie andere Vorgehensweisen zu migrieren.
\begin{itemize}
\item \textbf{Anpassbarkeit:} Möglichkeiten des unveränderten Systems Änderungen vorzunehmen.
\item \textbf{Installierbarkeit:} Systemvoraussetzung und Aufwand zur Installation des Systems.
\end{itemize}
\ \\
%
\textbf{Nichttechnische Kriterien}\\
Erweiterte Qualitätskriterien, welche nicht nach der DIN 9126 zugeordnet werden können.
\begin{itemize}
\item \textbf{Herstellerfirma und Produkt:} Dazu zählt die Marktposition, der Preis, die Produktplanung und der Service.
\end{itemize}
\ \\
%
Die zu untersuchenden Qualitätskriterien für die Softwareauswahl sind Funktionsumfang, Fehlertoleranz, Dokumentation, Zeitverhalten, Analysier- und Modifizierbarkeit.


\subsection{Qualitätsmetriken}
\label{qualitätsmetriken}
%TODO: essentielles markieren oder stärker wichten
Zu den wichtigen Qualitätsmerkmalen aus Kapitel \ref{softwarequalität} sind folgend Kriterien definiert, sowie je eine Bewertungsvorschrift und die geforderte Mindestbewertung für den Anwendungsfall angegeben.

\textbf{Richtigkeit:}\\
Berechnungen sind zu 100\% korrekt. Ausnahme ist dabei die Berechnung von Koordinaten. Dabei haben die Ergebnisse bis acht Stellen nach dem Komma korrekt zu sein.
Die statische Abbildung ist dabei $[korrekt, nicht\ korrekt]\ nach\ [1, 0]$ und die geforderte Mindestbewertung 1.

\textbf{Interoperabilität:}\\
Ist der Import und Export von räumlichen Daten aus PostgreSQL sowie eine Anbindungsmöglichkeit an den \Gls{umn}.
Statische Abbildung:\\
$[Datenschnittstelle\ und\ UMN\ Schnittstelle\ vorhanden,Datenschnittstelle\ vorhanden,$\\$UMN\ Schnittstelle\ vorhanden,keine\ Schnittstelle\ vorhanden]\ nach\ [12,7,5,0]$\\
Der Bereich bis 12 soll die Wichtigkeit des Vorhandenseins der Schnittstellen verdeutlichen.
Die geforderte Mindestbewertung ist 7.

\textbf{Funktionsumfang:}\\
Tabelle \ref{table:funktionsumfang} gibt die Wertung bei Existenz der einzelnen Funktionen wieder.
Existiert die Funktion nicht, ist die Wertung Null.
Ein Zwischenwert bei eingeschränkter Funktionalität ist möglich.
Maximale Wertung: 61 %\FPtrunc\Gesamtsumme\Gesamtsumme{0}\FPprint\Gesamtsumme\\
Es müssen mindestens geografische Datentypen, Filterfunktionen und parallele Verarbeitung vorhanden sein.
\begin{table}[h]
\centering
\begin{tabular}{l|c}
\textbf{Funktion} & \textbf{Wertung} \\ \hline
parallele Verarbeitung & \psum{2} \\ \hline
geografische Datentypen & \psum{14} \\ \hline
Umrechnung zwischen Koordinatensystemen & \psum{10} \\ \hline
Gruppierungsfunktionen & \psum{10} \\ \hline
Verschneidungsfunktionen & \psum{4} \\ \hline
Overlayfunktionen & \psum{4} \\ \hline
Geostatistik & \psum{6} \\ \hline
Filterfunktionen & \psum{10} \\ \hline
Schemaversionierung & \psum{1}
\end{tabular}
\caption{Wertungstabelle Funktionsumfang}
\label{table:funktionsumfang}
\end{table}

\textbf{Fehlertoleranz:}\\
Es gilt zu messen, ob Fehler bei einer Berechnung andere verschränkt gleichzeitig laufende Berechnungen beeinträchtigen.
Aus diesem Grund wird Unabhängigkeit auf eins und Abhängigkeit auf null abgebildet.
Die geforderte Mindestbewertung ist 1.

\textbf{Dokumentation:}\\
Vorhandene Dokumentation ist nach einzelnen Themen zu bewerten.
Dabei kann ein maximaler Wert von 13 erreicht werden.
\begin{table}[h]
\centering
\begin{tabular}{l|l}
\textbf{Dokumentation zu} & \textbf{Wertung je Eintrag} \\ \hline
Installation, Zeitverhalten & 1 \\ \hline
Funktionsumfang & 2 \\ \hline
Interoperabilität, Best practise, Anpassbarkeit & 3 
\end{tabular}
\caption{Wertungstabelle Dokumentation}
\label{table:dokumentation}
\end{table}
Die geforderte Mindestbewertung ist 6.

\textbf{Zeitverhalten:}\\
\label{bf:zeitverhalten}%
%TODO: Werte aus Ist-Stand in Bezug auf Lasttests verwenden
Konkret wird eine Beschleunigung aufwendiger Vorgänge angestrebt.
Die statische Abbildung auf Bezug auf die Laufzeiten des Ist-Standes:\\
$[Zeit\ wird\ \"uberschritten,\ Zeit\ ist\ gleich,\ Zeit\ wird\ unterschritten]\ nach\ [0,1,3]$\\
%Die Laufzeiten sind in Anhang \ref{appendix-A} zu finden.

\textbf{Modifizierbarkeit:}\\
Mögliche Anpassungen des Frameworks hinsichtlich der folgenden Punkte erhöhen den Wert um eins:\\
Verwendung eigener Datentypen, Erstellung eigener Schnittstellen, Erstellung eigener Funktionen, Verwendung der Programmiersprachen Scala oder R, Anlegen eigener Berechnungsvorgängen zur späteren Abarbeitung\\
Die geforderte Mindestbewertung ist abhängig vom Funktionsumfang und der Interoperabilität.
Jedoch sollten fehlende Funktionen und Schnittstellen erstellt werden können.

\subsection{Testfälle}
\label{subsection:testfaelle}
Zur definierten und wiederholbaren Erfassung der Erfüllung von speziellen Kriterien wie Funktionalität und Zeitverhalten,
folgt die Definition der Funktions- und Lasttests.

\subsubsection{Funktionstests}
Nach der Definition in Kapitel \ref{grundlagen-funktionstests} werden hiermit spezielle Funktionalitäten auf Vorhandensein und die Menge der Schnittstellen sowie der Austauschformate getestet.
Diese Funktionstest sind nicht automatisiert für unterschiedliche Frameworks durchführbar.
Deshalb werden sie manuell anhand der Spezifikation und des Quellcodes durchgeführt und die Ergebnisse tabellarisch festgehalten.

Folgende Schnittstellen sind zu testen:
\begin{itemize}
\item PostgreSQL
\item PostGIS
\item \Gls{umn}
\end{itemize}

Als Austauschformat ist mindestens eines der folgenden Datentypen zum Datenaustausch notwendig:
\begin{itemize}
\item Simple Feature Access
\item Objekte der JTS
\item PostGIS geometry
\end{itemize}

Speziell in GIS gilt es die Koordinatenreferenzsysteme zu analysieren.
Die EPSG Codes 3578 und 4326 werden im Anwendungsfall verwendet.

Für folgende Aufgaben sind die Funktionen zu analysieren:
\begin{itemize}
\setlength{\itemsep}{-8pt}
\item Umwandlung zwischen Koordinatensystemen
\item Verschneidung von räumlichen Daten
%\item Geostatistik
\item Interpolation
\item Kriging
\item topologische Filterung
\item räumliche Filterung
\end{itemize}
Dabei ist stets zu berücksichtigen mit welchen Datentypen die Funktionen verwendet werden können.

\subsubsection{Lasttests}
Die Basis für die Erstellung von Lasttests ist Kapitel \ref{iststand-vorgaenge}.
Eine exakte Definition der Lasttests ist in diesem Kapitel nicht möglich, da das zu verwendende Framework noch nicht ausgewählt wurde und somit die darin zu verwendenden Werkzeuge und Daten nicht bekannt sind.
Es werden jedoch die zu testenden Szenarien benannt.

Ein zu testendes Szenario ist die Aggregation von Daten.
Dabei ist eine Menge von mehreren hundert Megabyte an Punktdaten mit den gegebenen Werkzeugen abzurufen und die Laufzeit zu messen.
Die Werkzeuge sollten dabei eine Abfragesprache wie SQL und bei Vorhandensein der Schnittstelle der \Gls{umn} sein.
Über eine Abfragesprache sind neben der Laufzeit weitere Informationen abrufbar.
Mit SQL kann das Vorgehen des Query Planers ausgegeben werden, was für die Auswertung herangezogen werden kann.
Die Verwendung des \Gls{umn} zielt auf eine anwendungsnahe Darstellung der Leistungsfähigkeit ab.
Die hohe Leistungsfähig des \Gls{umn} für ein solches Szenario wurde bereits in \cite{ba:kurt} dargestellt, sodass von einer hohen Auslastung des \Gls{dbms} ausgegangen werden kann.

Analog zu diesem ersten Szenario ist ein weiterer zur Interpolation bzw. Kriging durchzuführen.
Darin sollen Punktdaten zu Vektordaten mit Interpolation im Rahmen eines Krigings verarbeitet werden.
Damit soll die Verarbeitungsleistung bewertet werden.

%Lasttests zur Persistierung von Eingangsdaten?

Da es sich bei den zu untersuchenden Frameworks um verteilte Systeme handelt, ist in allen Tests die Verteilung der Daten und der Verarbeitungsschritte zu berücksichtigen.


\section{Ist-Stand}
\label{IstStand}
%groben Ablauf textuell und grafisch darstellen
% geplanten Einsatz des Prototypen ebenso darstellen 

Die Durchführung einer Softwareauswahl zum teilweisen Ersatz eines bestehenden Systems setzt die Analyse des Systems voraus.
In diesem Unterkapitel wird der Ist-Stand sowie der der angestrebte Zustand nach Einbau des Prototypen dargestellt.
Firmeninterne Schnittstellen mit dem ist-Stand werden nicht konkretisiert, da sie den Anwendungsfall nicht schneiden.

In Abbildung \ref{fig:iststand} ist die Übersicht des Aufbaus ersichtlich.
Das Herzstück bildet die objektrelationale Datenbank PostgreSQL mit der geografischen Erweiterung PostGIS.
Diese dient zur Datenhaltung und wesentlich auch zur Datenverarbeitung.
In den Programmiersprachen Delphi und \Gls{r} werden zusätzlich automatische Verarbeitungsvorgänge durchgeführt.
Daten werden von extern und intern eingespeist.
Dabei handelt es sich um Punkte, Vektoren und Raster mit dazugehörigen Metadaten.
Im Rahmen der Datenverarbeitung werden Punktdaten interpoliert und mit Hilfe von Geostatistik Auswertungen aus originalen und verarbeiteten Daten erstellt.
Die Darstellung der Daten erfolgt über \Gls{umn} und \Gls{manifold}.
Als zentrales Element enthält die Datenbank neben den agrartechnischen Kennzahlen alle weiteren Daten des Unternehmens. In dieser Betrachtung werden einzig die agrartechnischen Kennzahlen berücksichtigt.
\begin{figure}[h]
\centering
\input{Abbildungen/Ist-Stand.tex}
\caption[Aktuelle Infrastruktur bei Agri Con]{Aktuelle Infrastruktur bei Agri Con}
\label{fig:iststand}
\end{figure}
%
\label{iststand-vorgaenge}

Es existiert eine Vielzahl von Vorgängen, welche zur Erhöhung der Useability in Hinsicht auf ihre Laufzeit verbessert werden können.
Für die Bewertung der Frameworks und schließlich zum Entwurf des Prototypen, ist es notwendig eine Auswahl der zu untersuchenden Vorgängen zu treffen.
Die Auswahl erfolgt durch Mitarbeiter der Agri~Con und hat Vorgänge mit der längsten Laufzeit als Ergebnis.
Dazu zählt das Laden von Daten zum Zwecke der Verarbeitung und Anzeige.
Die größten Datenmengen sind bei Punkten aus dem N-Sensor Bereich zu finden.
Die Anzeige der Punktdaten für einen Betrieb kann bis zu neun Sekunden dauern.
Punktdaten aus dem Docu Bereich werden ebenfalls nach mehreren Sekunden ausgegeben.
Die Punktdaten aus dem N-Sensor Bereich sind jedoch repräsentativ und werden deshalb betrachtet.
Weiterhin zählt Interpolation zu diesen Vorgängen.
Punkte und Fahrspuren werden mit einem angepassten Kriging interpoliert und als Vektoren oder Raster abgespeichert.
Dies wird mit einer R Bibliothek realisiert und bei großen Datenmengen Nachts angestoßen.
Das spezielle Kriging im jeweiligen Framework zu implementieren ist aufwendig, weshalb ein unveränderter Kriging Algorithmus für den Vergleich verwendet werden muss, sofern R nicht verwendet werden kann.\footnote{Dieser Algorithmus wurde im Auftrag durch dritte erstellt.}
Diese zwei charakteristischen Vorgänge sind durch ein Framework zu realisieren.

\begin{figure}[h!]
\centering
\input{Abbildungen/Wunsch-Stand.tex}
\caption[Aufbau Wunsch-Stand]{\"{U}bersicht des Aufbaus des Wunsch-Standes bei Agri Con}
\label{fig:wunschstand}
\end{figure}
Das Ziel ist es, ein Framework zusätzlich in den Ist-Stand zu integrieren.
Es soll dabei die aufwendigen Vorgänge durchführen, als Permanentspeicher für historische Daten dienen und Daten zur späteren Anzeige aufbereiten und bereitstellen.
Die aufwendigen Vorgänge werden in den Lasttests untersucht.
In welcher konkreten Form das Framework als Speicher für historische und aufbereitete Daten dient, ist abhängig vom Framework.
Auf Grund dessen ist dies nach Auswahl des Frameworks im Entwurf des Prototypen zu konkretisieren.

Für das Verständnis des Kapitels 6 wird nachfolgend das Datenbankschema knapp beschrieben.
Es handelt sich um ein umfangreiches Schema, weshalb es aus mehreren Schemata besteht:
adwork, apps, bu, bucardo, catalogs, checks, common, demo, docu, farm, files, import, information\_{}schema, ips, krig, log, n, nutrients, ogeo, pg\_{}catalog, pp, public, rax, statistics, tasks, temp, topology, umn, utils und yield.
catalogs enthält statische Einträge analog eines globalen Kataloges.
common enthält Benutzerspezifische, farm dagegen Betriebsspezifisce Daten.
Das Schema files beinhaltet Informationen zu Dateien, welche von Kunden auf den Server hochgeladen wurden sowie Metainformationen zu Dateiarten.
In den Schemata n und nutrients sind Daten zu N-Sensor zu finden.
Hilfsfunktionen und -tabellen des \Gls{umn} sind im Schema umn zu finden.
Alle Schemata beinhalten Funktionen, Trigger, Tabellen, Typen, Indizes und Contraints.
%- Grundschema der DB beschreieben (geografische agratechnische Daten und Daten des Unternehmens, der Organisation) -> ER mit dbvis?

%- quelldaten benennen und auf Anforderungen verweisen
%- Funktionsumfang hinsichtlich berechnung aufzeigen: was macht postgresql und was R
%- 


\section{Stand der Forschung}
%Diskussion der Literatur
Die jährlich stattfindende Internationale Messe FOSSGIS\footnote{Freie und Open Source Software für Geoinformationssysteme \url{https://www.fossgis.de/}} zeigte das PostgreSQL mit PostGIS in der Regel als freies \Gls{dbms} für GIS eingesetzt wird.
Zwar existiert eine Vielzahl von Elementen zur Darstellung und Kartenverwaltung% erzeugt aus den vorhandenen Daten
, aber es scheint keine geeignete Alternative zur Datenhaltung und Verarbeitung von komplexen geografischen Daten zu geben.

Die Literaturrecherche bestätigt dies.\\
So vergleicht Ahlers in seiner Bachelorarbeit\footnote{\cite{ba:pgvsoracle}} PostGIS mit Oracle Spatial, da auch für ihn PostGIS der größte freie Vertreter eines GIS ist.\\
Thurm untersucht in \cite{ba:nosqlfuergeodaten} gängige NoSQL \Gls{dbms} für Eignung mit geografischen Daten im Vergleich mit PostGIS und Oracle Spatial.
Darin kommt der Autor zu dem Schluss, dass es für einfache Datenobjekte in begrenzten Mengen nützlich ist die NoSQL Alternativen in Betracht zu ziehen.
Beispielsweise eignet sich die graphenbasierte Datenbank Neo4J für Operationen mit Strecken.
Für weniger spezielle und mehr komplexe Anforderungen sind einzig PostGIS und Oracle Spatial zu empfehlen.
Die NoSQL Systeme im Bereich der Datenverarbeitung von räumlichen Daten sind in den letzten Jahren entstanden und somit nicht ausgereift und umfangreich, so Thurm auf S.51.\\
\cite{ma:neo4j} vergleicht dagegen Neo4J mit PostGIS und kommt auch zu dem Schluss, dass graphenbasierte Datenhaltung zwar Vorteile besitzt, aber die Eignung individuell validiert werden muss.

Wissenschaftliche Dokumente in Form von Paper sind zu diesem Thema vorhanden.
Diese beschreiben aber nur knapp Entwürfe eines Systems für einen speziellen Anwendungsfall, weshalb sie in diesem Zusammenhang nicht gebraucht werden können.
\cite{paper:hdfsspatial} beschreibt einen Entwurf zur Speicherung und Verarbeitung räumlichen Daten mit Hadoop, VegaGiStore genannt.
\cite{paper:spatialdistribution} erläutert dagegen Methoden zur Verteilung von Daten in einem verteilten System auf Grund der räumlichen Informationen.

Es existieren Arbeiten zu Randthemen und -betrachtungen sowie Teilproblemen, aber nicht zu den Fragen \textit{Gibt es eine freie Alternative zu PostGIS?} und \textit{Welche verteilten GIS eignen sich für einen komplexen Anwendungsfall?}.

Neben wissenschaftlichen Dokumenten finden sich Werbetexte zu scheinbar geeigneten Systemen.
Das Fehlen von verlässlichen konkreten Informationen erschwert die Literaturrecherche.
Speziell Leistungsvergleiche sind in diesem Zusammenhang interessant, sind aber auf Grund fehlender Informationen nicht reproduzierbar.
Nachfolgend zwei Beispiele einer solchen Werbung:\\
%Unternehmen werben mit höher Leistung als PostGIS:\\
GeoMesa: Zehn bis 60 fache Antwortgeschwindigkeit bei GeoMesa gegenüber PostGIS, entsprechend der Abbildung \ref{fig:geomesaversuspostgis}.
\begin{figure}[h!]
\centering
\includegraphics[width=.8\textwidth]{Abbildungen/geomesa_versus_postgis.png}
\caption[Antwortzeiten von GeoMesa und PostGIS]{Antwortzeiten von GeoMesa und PostGIS, Quelle \cite[S.24]{website:slideshare:geomesa}}
\label{fig:geomesaversuspostgis}
\end{figure}
\newpage
Postgres-XL: Bis zu sechs fache Antwortdauer von PostgreSQL gegenüber Postgres-XL in einem standardisierten TPC-H\footnote{\url{http://www.tpc.org/tpch/}} Benchmark bei Verwendung von vier DataNodes. Abbildung \ref{fig:pgxcversuspgsql} zeigt das Diagramm mit dem genannten Wert. Die x-Achse ist nicht beschrieben, weswegen eine Einschätzung der Werte nicht möglich ist.
\begin{figure}[h!]
\centering
\includegraphics[width=.8\textwidth]{Abbildungen/postgresxl_versus_postgis.png}
\caption[TPC-H Benchmark von PostgreSQL und Postgres-XL]{TPC-H Benchmark von PostgreSQL und Postgres-XL, Quelle \cite[S.12]{website:slideshare:pgxc}}
\label{fig:pgxcversuspgsql}
\end{figure}

%nichts da, weswegen interesant zu erforschen