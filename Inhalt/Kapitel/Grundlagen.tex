\chapter{Grundlagen}
\Gls{computer}

\section{Datenbank}

\subsection{ACID}
\Gls{acid}

\subsection{MVCC}
\Gls{mvcc}

\subsection{BASE}
\Gls{base}

\subsection{weitere Begriffsdefinitionen}

\subsection{Indexstrukturen}

\subsubsection{R-Baum}

\subsubsection{B-Baum}

\subsubsection{LSM-Baum}

\subsubsection{Geohash}

\subsection{Mehrrechner-Datenbanksystem}

\subsection{Verteiltes Datenbanksystem}

\subsection{Replikationsverfahren}

\subsubsection{Synchron}

\subsubsection{Asynchron}

\subsubsection{Kaskadiert}

\newpage

\section{geografische Datenverarbeitung}

\subsection{Bezugssysteme}

\subsection{Datenformate}

\subsubsection{Punkte}

\subsubsection{Vektoren}

\subsubsection{Raster}

\subsubsection{Shapefile}

\subsection{GIS}

\subsection{PostGIS}

\subsection{GeoTools}



\section{NoSQL}

\subsection{Definition}

\subsection{Kategorisierung}

\subsection{Hadoop}
% http://blog.samibadawi.com/2012/03/hive-pig-scalding-scoobi-scrunch-and.html

Hadoop ist ein unter der Apache Lizenz 2.0 stehendes Java-Framework zur Datenhaltung und Verarbeitung von großen Datenmengen auf mehrerern Computern.
Es basiert auf MapReduce und dem Dateisystem HDFS.

\Gls{hdfs} ist ein verteiltes Dateisystem, welches keine besonderen Anforderungen an die Hardware stellt und für die Verwendung von mehreren hundert bis tausend Computern\footnote{die in einem verteilten System teilnehmenden Computer heißen Knoten} ausgelegt ist.
Es besitzt eine hohe Fehlertoleranz und ist für den Einsatz auf kostengünstiger Hardware ausgelegt.
Hoher Datendurchsatz und die Verwendung großer Dateien\footnote{eine Datei kann mehrere Gigabyte bis mehrere Terrabyte groß sein und wird in Blöcke gleicher Länge aufgeteilt} sind wesentliche Merkmale.[vgl. \cite{paper:hadoop} S. 3]
Die Datei-Blöcke werden redundant auf die Knoten verteilt und sind mit Hilfe des Name-Node abrufbar.[vgl. \cite{ba:dan} S. 7]

Die verteilte Verarbeitung übernimmt MapReduce.
Entsprechend dem Namen entspringt der Name MapReduce aus der funktionalen Programmierung, in welcher die Funktionen \glqq map\grqq \ und \glqq reduce\grqq \ zum Einsatz kommen.
So werden hier die Daten mit einer map-Funktion verändert und mit reduce-Funktion aggregiert.
Ein Master weist die Daten und Funktionen den Slaves\footnote{In diesem Zusammenhang auch Worker genannt} zu.
Die Slaves führen die Funktionen mit den ihnen zugewiesenen Daten aus und speichern ihre Ergebnisse auf deren Festplatte ab.
MapReduce wurde von Google definiert.
Auch hier werden keine besonderen Anforderungen an die Hardware gestellt.[vgl. \cite{paper:mapreduce} S. 3]


Hadoop besitzt eine Master-Slave Architektur, wobei der Name-Node\footnote{damit ist der Master-Knoten gemeint, auch Jobtracker genannt} ankommende Anfragen bearbeitet und die Slave-Knoten organisiert.
Hadoop ist per API verwendbar und bietet sich somit zur Stapelverarbeitung an. %Todo: belegen
Es wird meist nur als Grundgerüst verwendet und mit Datenbanken wie HBase, MongoDB oder PostgreSQL sowie mit Frameworks für die Nutzung wie Hive, Pig, Spark oder Scalding erweitert.





\subsection{Accumulo}
\url{https://en.wikipedia.org/wiki/Apache_Accumulo}

\subsection{NoSQL GIS}

\subsection{MongoDB}

\subsection{CouchDB}

\subsection{Neo4J}

\newpage

\subsection{Rasdaman}

\url{http://live.osgeo.org/de/overview/rasdaman_overview.html} :\\
- Array-Datenbanksystem
- PostgreSQL Aufsatz
- Multi-Dimensionalität
- eigene Anfragesprache
- skalierend
- unterstützt WCS Core und WCPS
- Implementierte Standards: OGC WMS 1.3, WCS 2.0, WCS-T 1.4, WCPS 1.0, WPS 1.0
- Lizenz: Clients und APIs: GNU Lesser General Public License (LGPL) version 3; Server-Engine: GNU General Public License (GPL) version 3
- Unterstützte Plattformen: Linux, MacOS, Solaris
- APIs: rasql, C++, Java


\url{http://www.rasdaman.org/} :\\
- open-source
- "extends standard relational database systems with the ability to store and retrieve multi-dimensional raster data"


\url{http://www.rasdaman.de/} :\\
- "erlaubt die Ablage von unbeschränkt grossen multi-dimensionalen Arrays ("Rasterdaten") in einer konventionellen Datenbank"


\subsection{Spacebase}

\url{http://docs.paralleluniverse.co/spacebase/} :\\
- serverseitig
- in-memory
- spatial data store
- ausgelegt für viele rechner und hohen Durchsatz (real-time)
- 2D und 3D Objekte mit 3D bbox
- load balancing enthalten
- spatial querys möglich
- benötigt JVM
- API für Java, Ruby, Python, Node.js, C++, Erlang
- API stellt nur elementare spatial querys zur verfügung: intersect oder contains
- eigene spatial querys können definiert werden

\subsection{Geomesa}

- Ingest = Import über Kommandozeile (geomesa-tools)
- Ingest von shp, csv und tsv Dateien
- Anderer Dateiimport mit GeoTools
- Verarbeitung nur über externe Tools (Spark, geotools)
- Export: csv, tsv, shp, geojson, gml

\url{http://www.eclipse.org/community/eclipse_newsletter/2014/march/article3.php} :\\
- open-source
- build on Accumulo and Hadoop
- Supporting the GeoTools API
- GeoServer Plugin
- geohash for indexing


\url{https://www.locationtech.org/proposals/geomesa} :\\
- outperforming postgis with geoserver


\url{http://de.slideshare.net/CCRinc/location-techdc-talk2-28465214}
- Verwendung fraktaler Kurven
- mit Spark und Scalding wesentlich schneller als PostGIS


\url{https://docs.google.com/presentation/d/1NO0ppk8MfDs8Q-QcUidZCSZK7YYwd9RjJoHV1V4Yq_w/edit?pli=1#slide=id.p} :\\
- 

%storm vs spark: http://xinhstechblog.blogspot.de/2014/06/storm-vs-spark-streaming-side-by-side.html https://stackoverflow.com/questions/24119897/apache-spark-vs-apache-storm http://www.zdatainc.com/2014/09/apache-storm-apache-spark/


\url{https://github.com/locationtech/geomesa}:\\
- Apache License Version 2.0


\subsection{ESRI GIS Tools for Hadoop}
- 4 elements\\
- Esri Geometry API for Java: "This is a generic library that includes geometry objects, spatial operations, and spatial indexing, it can be used to spatially enable Hadoop. By deploying the Esri geometry API library (as a jar) within Hadoop, developers are able to build Map/Reduce applications that are spatially enabled, by leveraging the Esri Geometry API along with the other Hadoop APIs in their application."\cite{website:esri-hadoop2}\\
- Spatial Framework for Hadoop: "This library includes the user defined objects that extend Hive with the capabilities of the Esri Geometry API. By enabling this library in Hive, users are able to construct queries that are very SQL like using HQL. In this case, users don’t have to write a Map/Reduce application, they can interact with Hive, write their SQL like queries and get answers directly from Hadoop. Queries in this case can include spatial operations and values."\cite{website:esri-hadoop2}\\
- Geoprocessing Tools for Hadoop: "These tools are specifically used in ArcGIS. Through the tools, users can connect to Hadoop from ArcGIS. Connecting to Hadoop from ArcGIS is really useful to the toolkit users, since they can import their analysis result in ArcGIS for Visualization. They can also do more complex and sophisticated analysis now that they narrowed down their data to a specific subset. Additionally, users can leverage the ArcGIS platform capabilities to publish their maps to web and mobile apps, and can integrate it with BI reports."\cite{website:esri-hadoop2}\\
- GIS Tools for Hadoop: "This project is intended as a place to include multiple samples that leverage the toolkit. The samples can leverage the low level libraries, or the Geoprocessing tools. A couple of samples are available to help you test the deployment of the spatial libraries with Hadoop and Hive, and make sure everything runs with no issues before you start leveraging the setup from your HQL queries, or from the GP tools. To check your deployment, for Hive and GP tools usage, the sample point-in-polygon-aggregation-hive can be utilized. The sample leverages the data and lib directories on the same path."\cite{website:esri-hadoop2} (Benötigt ArcGIS)\\
- Apache License, Version 2.0

\section{Leistungstests}

- siehe BA
- in Absprache mit Prof. Riechert